library(tm)
library(cluster)
setwd("C:/Users/Vishak/Documents/Code/R")
# Reaading the data from the text file
data <- read.csv("Big1.csv",header = T,stringsAsFactors = F)
# Converting data to utf-8 encoding
data <- iconv(data,to = "utf-8")
# Creating a corpus where each line is taken as a document
corpus <- Corpus(VectorSource(data))
# Removing punctuation marks in the corpus
# Generating the document term matrix
dtm <- DocumentTermMatrix(corpus)
# As we have taken the corpus from a text file the rownames are stored as character(0)
# We have to change it to the tweet_id or tweet_number
rownames(dtm) <- c(1:nrow(dtm))
# Generating the matrix form of dtm
m <- as.matrix(dtm)
# Sorting the matrix in Descending order
v <- sort(colSums(m),decreasing=TRUE)
# Converting 'v' into a data frame and storing it in the variable 'd'
d <- data.frame(word = names(v),freq=v)
# Here the row names will be same as the words,so setting the row names to NULL
rownames(d)<-NULL
# Finding the frequent terms with minimum number of occurences as 50
freq_terms <- findFreqTerms(dtm,lowfreq = 50)
# Finding the associations of these frequent words with the corlimit as 0.25
# Clustering using k-means
# creating a sparse matrix with the percentage of sparsity as 90%
dtms <- removeSparseTerms(dtm,0.9)
# k-means clustering using euclidian as a measurement method
d <- dist(t(dtms), method="euclidian")
freq_terms
library(tm)
library(cluster)
# Reaading the data from the text file
data <- read.csv("mergefile.csv",header =F,stringsAsFactors = F)
# Converting data to utf-8 encoding
data <- iconv(data,to = "utf-8")
# Creating a corpus where each line is taken as a document
corpus <- Corpus(VectorSource(data))
# Generating the document term matrix
dtm <- DocumentTermMatrix(corpus)
# As we have taken the corpus from a text file the rownames are stored as character(0)
# We have to change it to the tweet_id or tweet_number
rownames(dtm) <- c(1:nrow(dtm))
# Generating the matrix form of dtm
m <- as.matrix(dtm)
# Sorting the matrix in Descending order
v <- sort(colSums(m),decreasing=TRUE)
# Converting 'v' into a data frame and storing it in the variable 'd'
d <- data.frame(word = names(v),freq=v)
# Here the row names will be same as the words,so setting the row names to NULL
rownames(d)<-NULL
# Finding the frequent terms with minimum number of occurences as 50
freq_terms <- findFreqTerms(dtm,lowfreq = 50)
# Writing the data frame 'd' into 'dtm.csv' for further use
write.csv(d,"dtm.csv",row.names = F)
# Clustering using k-means
# creating a sparse matrix with the percentage of sparsity as 90%
dtms <- removeSparseTerms(dtm,0.9)
head(inspect(dtms))
# k-means clustering using euclidian as a measurement method
d <- dist(t(dtms), method="euclidian")
# Finding k-means
#kfit <- kmeans(d, 4)
sink("cluster_centers.txt")
kfit$centers
sink()
freq_terms
library(tm)
library(SnowballC)
library(wordcloud)
wordcloud(corpus, max.words = 100, random.order = FALSE)
q()
